{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Open AI gym"
      ],
      "metadata": {
        "id": "fsjDSRTLYHZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym"
      ],
      "metadata": {
        "id": "oUT9xnBKYAQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import necessary packages: gym, numpy, random, matplotlib"
      ],
      "metadata": {
        "id": "tLmSELoXYMaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "GH4IBB7nXmHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Open AI Gym's Cliffwalking environment"
      ],
      "metadata": {
        "id": "avoPsIiQXtLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CliffWalking-v0')"
      ],
      "metadata": {
        "id": "1IT23YmhXyrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize variables"
      ],
      "metadata": {
        "id": "h4iDJpoOYwt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 50 # number of steps\n",
        "episode_num = 1000 # number of episodes\n",
        "gamma = 1\n",
        "alpha = 0.1\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "bx2U97MOX9_D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Q(s,a) arbitrarily\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "# initialize random policy\n",
        "policy = np.zeros(num_states)\n",
        "for s in range(num_states):\n",
        "    policy[s] = env.action_space.sample()\n",
        "# initialize cumulative reward list\n",
        "cumulative_reward_list = np.zeros(episode_num)"
      ],
      "metadata": {
        "id": "0d0smSHYZsWX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Q-learning"
      ],
      "metadata": {
        "id": "-xYRiv5Yizrj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0QuRYxZFZ84I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot results"
      ],
      "metadata": {
        "id": "S9J44tpBZqvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([*range(episode_num)],cumulative_reward_list)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title('Reward per Episode in Cliffwalking Environment with Tabular Q-learning')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vA5IYB1nZp4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}